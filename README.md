# Discussion


## TODO
 - [ ] Combine Longformer with XL
 - [ ] Block Recurrent Transformer from scratch
 - [ ] Finish Evaluation for Gallery (Perplexity etc...)
 - [ ] Experiment - Longformer @Yuetian

## Resources


### TransformerXL
 - arXiv Paper: https://arxiv.org/abs/1901.02860
 - 

### Longformer
 - arXiv Paper: https://arxiv.org/abs/2004.05150
 - Longformer: The Long-Document Transformer (Yannic Kilcher): https://www.youtube.com/watch?v=_8KNb5iqblE
 - How much memory does Longformer use? (Yannic Kilcher): https://www.youtube.com/watch?v=gJR28onlqzs
 - Longformer Blog: https://towardsdatascience.com/longformer-the-long-document-transformer-cdfeefe81e89


### Big Bird
 - Huggingface: https://huggingface.co/blog/big-bird
 - Variants of attention: https://huggingface.co/blog/big-bird


### Block Recurrent Transformer
 - arXiv Paper: https://arxiv.org/abs/2203.07852


### Other
 - One Write-Head is All You Need: https://arxiv.org/abs/1911.02150
 - Flash Attention: https://arxiv.org/abs/2205.14135
 - Diagonal State Space Models for long sequences: https://arxiv.org/abs/2206.11893
 - 

### Repo

 [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=augustwester&repo=transformer-xl)](https://github.com/augustwester/transformer-xl)

 [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=lucidrains&repo=block-recurrent-transformer-pytorch)](https://github.com/lucidrains/block-recurrent-transformer-pytorch)
 
 [![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=allenai&repo=longformer)](https://github.com/allenai/longformer)


## Links

